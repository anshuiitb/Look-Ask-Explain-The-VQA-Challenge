{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ad676d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998641014099121}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "print(classifier(\"I love Hugging Face!\"))  # [{'label': 'POSITIVE', 'score': 0.999}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ab7e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9598047137260437}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}, {'label': 'NEGATIVE', 'score': 0.999782383441925}]\n"
     ]
    }
   ],
   "source": [
    "print(classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\",\"bad\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a1310b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['study', 'education', 'business'],\n",
       " 'scores': [0.7680259943008423, 0.20481902360916138, 0.027154982089996338]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"study\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8269361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "GPU name: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a807ecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd86acda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello mam , you are not allowed here. This is a restricted area and there is no way to leave. I will not take any information from you, you are not allowed here. I will not ask you questions from others who you may be, just ask for help. You may not talk to anyone or be seen by anyone. Do not ask me questions that are not to your mind. If you have any questions please feel free to contact me. I will try to answer them as quickly as possible. If you need any more help please let me know! I'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\"\n",
    ")\n",
    "\n",
    "pipe(\"Hello mam , you are not allowed here. This is a restricted area\", max_new_tokens=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49b12a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'spotify',\n",
       " 'labels': ['music', 'voilence', 'anger', 'business'],\n",
       " 'scores': [0.7156035304069519,\n",
       "  0.18480663001537323,\n",
       "  0.04997563734650612,\n",
       "  0.04961423575878143]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"spotify\",\n",
    "    candidate_labels=[\"anger\", \"business\",\"voilence\",\"music\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7b0ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19619710743427277,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.040527042001485825,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc788602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2596301734447479,\n",
       "  'token': 1648,\n",
       "  'token_str': 'role',\n",
       "  'sequence': 'This course will teach you all about role models.'},\n",
       " {'score': 0.09427272528409958,\n",
       "  'token': 1103,\n",
       "  'token_str': 'the',\n",
       "  'sequence': 'This course will teach you all about the models.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\",model=\"google-bert/bert-base-cased\")\n",
    "unmasker(\"This course will teach you all about [MASK] models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d47811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "c:\\Users\\anshu\\anaconda3\\envs\\hf-modern\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9796019),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8669a31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': np.float32(0.9986273),\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'B-PER',\n",
       "  'score': np.float32(0.93460405),\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'B-PER',\n",
       "  'score': np.float32(0.7915613),\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'B-PER',\n",
       "  'score': np.float32(0.90470827),\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': np.float32(0.9670036),\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'B-ORG',\n",
       "  'score': np.float32(0.8853456),\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9884615),\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': np.float32(0.9971419),\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "nlp(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503f6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'America has changed dramatically during recent years. The number of graduates in traditional engineering disciplines has declined. There are declining offerings in engineering subjects dealing with infrastructure, the environment, and related issues. Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "summarizer(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7bdba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anshu\\anaconda3\\envs\\hf-modern\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e11658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Этот курс продюсирует Хаггинг Фэйс.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-ru\")\n",
    "translator('This course is produced by Hugging Face.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011f3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.9.0+cu130\n",
      "CUDA available: True\n",
      "CUDA version: 13.0\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# print(\"Torch:\", torch.__version__)\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# print(\"CUDA version:\", torch.version.cuda)\n",
    "# print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99aa7638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your word is: ['nigga, we good?', 'n word'], sentiment :[{'label': 'POSITIVE', 'score': 0.9996993541717529}, {'label': 'NEGATIVE', 'score': 0.7045801281929016}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\" , model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "text=[\"nigga, we good?\",\"n word\"]\n",
    "sentiment_ = sentiment_analyzer(text)\n",
    "print(f\"your word is: {text}, sentiment :{sentiment_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c398b494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55773af9018b415986c221d566f728e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/814 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anshu\\anaconda3\\envs\\hf-modern\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anshu\\.cache\\huggingface\\hub\\models--philschmid--distilbert-base-multilingual-cased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993fb5f6c2c748eb8ea6f16b12266787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34ca1265d2446a3bb49246abbf3ac8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938c0e9131a94d8c8f932902b8b45615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f7197d60b648b793b882dccfc5169f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296818076842472c882a6b9e4c02a1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a700125ac2fc4cb385af282c48fc0cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "spam_classifier = pipeline(\"text-classification\",model=\"philschmid/distilbert-base-multilingual-cased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e06ea58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9878117442131042},\n",
       " {'label': 'positive', 'score': 0.5534405708312988},\n",
       " {'label': 'negative', 'score': 0.7591308951377869}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_texts = [\"congratulations! you won a 500 INR Amazon gift card. Click here to claim now.\",\n",
    "              \"Hi Amit, let's have a meeting tommorow at 12 PM.\",\n",
    "              \"Someone loged in through your gmail, click here to know more.\"]\n",
    "results = spam_classifier(spam_texts)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a43d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM,AutoTokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c74e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_summary = \"\"\"\n",
    "Virat Kohli (born 5 November 1988)[a] is an Indian international cricketer and the former all-format captain of the Indian national cricket team.[3] He is a right-handed batter and occasional right-arm medium pace bowler. Considered one of the greatest all-format batsmen in the history of cricket, he has been nicknamed the King, the Chase Master, and the Run Machine for his skills, records and ability to lead his team to victory.[4] Kohli has the most centuries in ODIs and the second-most centuries in international cricket with 84 tons across all formats. He is also the leading run-scorer in the Indian Premier League.[5] Kohli is the most successful Test captain of India with most wins and 3 consecutive Test mace retainments.[6] He is the only batter to earn 900+ rating points across all 3 formats.[7]\n",
    "\n",
    "Kohli was the captain of the 2008 U19 World Cup winning team and was a crucial member of the teams that won 2011 ODI World Cup, 2013 Champions Trophy, 2024 T20 World Cup, and 2025 Champions Trophy. He plays for Royal Challengers Bengaluru in the Indian Premier League and for Delhi in domestic cricket. In 2013, Kohli was ranked number one in the ODI batting rankings. In 2015, he achieved the same in T20I.[8] In 2018, he was ranked number one in Test, making him the only Indian to hold the number one spot in all three formats. He is the first player to score 20,000 runs in a decade. He was the Cricketer of the Decade for 2011 to 2020.[9]\n",
    "\n",
    "Kohli has won ten ICC Awards, making him the most awarded player in international cricket history. He won the ODI Player of the Year award four times in 2012, 2017, 2018, and 2023. He won the Cricketer of the Year award, on two occasions, in 2017 and 2018. In 2018, he became the first player to win all three major awards including Cricketer of the Year, ODI Player of the Year and Test Player of the Year in the same year. He was honored with the Spirit of Cricket Award in 2019 and given the Cricketer of the Decade and ODI Cricketer of the Decade in 2020. Kohli was named the Wisden Leading Cricketer in the World for three consecutive years.\n",
    "\n",
    "Kohli has the most Player of the Series and second most Player of the Match awards to his name in all three formats combined. He was honoured with the Arjuna Award in 2013, the Padma Shri in 2017, and India's highest sporting honour, the Khel Ratna Award, in 2018. Time included him on its 100 most influential people in the world list in 2018. Kohli has been deemed one of the most commercially viable athletes, with estimated earnings of ₹634 crore (US$75 million) in the year 2022.\n",
    "\n",
    "After winning the 2024 T20 World Cup and winning the Player of the Match award in the final, Kohli announced his retirement from T20Is.[10] On 12 May 2025, aged 36, he announced his retirement from the Test format.[11] He is married to actress Anushka Sharma, and they have two children named Vamika and Akaay Kohli.[12]\n",
    "\n",
    "Early life\n",
    "Kohli was born on 5 November 1988 in Delhi into a Punjabi Hindu family. His mother Saroj Kohli is a homemaker while his father Prem Nath Kohli worked as a criminal lawyer. He has an elder brother Vikas and an elder sister Bhawna. His formative years were spent in Uttam Nagar. His early education was at Vishal Bharti Public School.[13] As per his family, Kohli exhibited an early affinity for cricket as a 3-year-old. He would pick up a bat and request his father bowl to him.[14] In 1998, the West Delhi Cricket Academy was created. In May, his father arranged for him to meet Rajkumar Sharma.[15] Upon the suggestion of their neighbours, Kohli's father considered enrolling his son in a professional cricket academy, as they believed his ability merited more than gully cricket.[16]\n",
    "\n",
    "Kohli was unable to secure a place in the U-14 Delhi team, due to extraneous factors. His father reportedly received offers to relocate his son to influential clubs, which would ensure his selection, but he declined the proposals. Kohli eventually found his way into the U-15 team.[17] He received training at the academy and participated in matches at the Sumeet Dogra Academy located at Vasundhara Enclave.[18] To further his cricket career, he transferred to Saviour Convent School during his ninth grade.[16] On 18 December 2006, his father died due to a cerebral attack.[19] As per his mother, Kohli's demeanour shifted noticeably after his father's death. He took on cricket with newfound seriousness, prioritizing playing time and dedicating himself fully to the sport.[16] Kohli's family resided in Meera Bagh, Paschim Vihar until 2015, after which they relocated to Gurgaon.[20]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c253ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(\"ummarize: \"+ text_for_summary , return_tensors=\"pt\" , max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c76d9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_id = model.generate(inputs , max_length = 50 , min_length=25 , length_penalty=10.0 , num_beams=4)\n",
    "summary=tokenizer.decode(summary_id[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f49278c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Virat Kohli (born 5 November 1988) is an Indian international cricketer and the former all-format captain of the Indian national cricket team. He is a right-handed batter and occasional right-arm medium pace bow'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f669b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067a89759ff433a9218bc57867faa8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anshu\\anaconda3\\envs\\hf-modern\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anshu\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c593e542d9974520b0f47470835bb16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff715da94cb24af3881e7e9987cbc01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c14f1801af4eb3abcf802f99f2a3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab087c8ed1bc4b2fab5f41cf07f0d28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering,AutoTokenizer\n",
    "\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model= AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afada529",
   "metadata": {},
   "outputs": [],
   "source": [
    "context =\"\"\" I am amit diwan from delhi,India. mu interests are x,y,z. I have studied courses not only in programming , but web dev also \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: delhi, india\n"
     ]
    }
   ],
   "source": [
    "question=\"where is amit diwan is based?\"\n",
    "inputs = tokenizer(question,context,return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "start_index=start_scores.argmax()\n",
    "end_index = end_scores.argmax()\n",
    "\n",
    "answer_tokens = inputs[\"input_ids\"][0][start_index:end_index +1]\n",
    "answer = tokenizer.decode(answer_tokens , skip_special_tokens=True)\n",
    "\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6836de98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab06794f5aa42a4b3a5f0789d8bde25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Use your Hugging Face token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c9dcc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'user', 'id': '685a9db4a4d6eb5bd7a377f0', 'name': 'GOD0007', 'fullname': 'anshu yadav', 'email': 'anshuyadav1223334444@gmail.com', 'emailVerified': True, 'canPay': False, 'billingMode': 'prepaid', 'periodEnd': 1769904000, 'isPro': False, 'avatarUrl': '/avatars/bda04ad54bc92dfa83a8390a0d0eb41e.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'notebooks', 'role': 'write', 'createdAt': '2026-01-06T09:54:08.607Z'}}}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23e261",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-modern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
