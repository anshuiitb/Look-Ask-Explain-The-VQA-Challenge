{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4707d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('word2vec-google-news-300') \n",
    "# downloads ~1.6GB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400e9082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
      " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
      "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
      " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
      "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
      "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
      "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
      "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
      "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
      "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
      "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
      "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
      " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
      " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
      " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
      "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
      "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
      " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
      " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
      " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
      "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
      "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
      "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
      " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
      " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
      "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
      " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
      "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
      " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
      " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
      "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
      " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
      " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
      "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
      " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
      "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
      "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
      " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
      " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
      " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
      " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
      " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
      "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
      "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
      "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
      " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
      "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
      "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
      " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
      " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
      " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
      "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
      "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
      " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
      "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
      " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
      "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
      "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
      " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
      "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
      "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
      "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
      "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
      "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
      " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
      "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
      " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
      "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
      "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
      " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
      "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
      "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
      "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
      " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
      " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n",
      "[('kings', 0.7138045430183411), ('queen', 0.6510956883430481), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474), ('sultan', 0.5864824056625366), ('ruler', 0.5797567367553711), ('princes', 0.5646552443504333), ('Prince_Paras', 0.5432944297790527), ('throne', 0.5422105193138123)]\n"
     ]
    }
   ],
   "source": [
    "print(model['king'])  # see vector\n",
    "print(model.most_similar('king'))  # analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16844d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Most similar to 'dog':\n",
      "[('dogs', 0.8680489659309387), ('puppy', 0.8106428384780884), ('pit_bull', 0.780396044254303), ('pooch', 0.7627376914024353), ('cat', 0.7609457969665527), ('golden_retriever', 0.7500901818275452), ('German_shepherd', 0.7465174198150635), ('Rottweiler', 0.7437615394592285), ('beagle', 0.7418621778488159), ('pup', 0.740691065788269)]\n",
      "\n",
      "üëë Word analogy: king - man + woman = ?\n",
      "[('queen', 0.7118193507194519)]\n",
      "\n",
      "üìè Similarity between 'coffee' and 'tea':\n",
      "0.5635292\n",
      "\n",
      "üîé Is 'dragon' in vocabulary?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# # Load the pre-trained model (you‚Äôve already done this probably)\n",
    "# model_path = \"/content/GoogleNews-vectors-negative300.bin.gz\"\n",
    "# model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "# 1. Most similar words\n",
    "print(\"üìç Most similar to 'dog':\")\n",
    "print(model.most_similar('dog'))\n",
    "\n",
    "# 2. Word analogy: king - man + woman = ?\n",
    "print(\"\\nüëë Word analogy: king - man + woman = ?\")\n",
    "print(model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
    "\n",
    "# 3. Word similarity\n",
    "print(\"\\nüìè Similarity between 'coffee' and 'tea':\")\n",
    "print(model.similarity('coffee', 'tea'))\n",
    "\n",
    "# 4. Check if a word exists\n",
    "print(\"\\nüîé Is 'dragon' in vocabulary?\")\n",
    "print('dragon' in model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d0a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Sentence Vector (shape): (300,)\n",
      "üìà First 5 dimensions: [ 0.01123047 -0.01138306  0.02069092  0.14361572 -0.03967285]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# # Load Google News vectors\n",
    "# model_path = \"/content/GoogleNews-vectors-negative300.bin.gz\"\n",
    "# model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "def avg_word2vec(sentence, model):\n",
    "    words = sentence.lower().split()\n",
    "    valid_vectors = [model[word] for word in words if word in model]\n",
    "    \n",
    "    if not valid_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    return np.mean(valid_vectors, axis=0)\n",
    "\n",
    "# Try it!\n",
    "sentence = \"I love machine learning\"\n",
    "vector = avg_word2vec(sentence, model)\n",
    "\n",
    "print(\"üî¢ Sentence Vector (shape):\", vector.shape)\n",
    "print(\"üìà First 5 dimensions:\", vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5266ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'learn' and 'learning' using BERT: 0.930\n",
      "Similarity between 'india' and 'indian' using BERT: 0.957\n",
      "Similarity between 'fame' and 'famous' using BERT: 0.956\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n",
    "\n",
    "# Compute similarity for each pair of words\n",
    "for pair in word_pairs:\n",
    "    tokens = tokenizer(pair, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    # Extract embeddings for the [CLS] token\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    similarity = torch.nn.functional.cosine_similarity(cls_embedding[0], cls_embedding[1], dim=0)\n",
    "    print(f\"Similarity between '{pair[0]}' and '{pair[1]}' using BERT: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977113c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_name",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
